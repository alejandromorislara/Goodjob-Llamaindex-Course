{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Uso de APIs con LlamaIndex**  \n",
        "\n",
        "Este notebook forma parte del curso de **IA Generativa** de la [FundaciÃ³n GoodJob](https://www.fundaciongoodjob.org/).  \n",
        "En Ã©l, los alumnos aprenderÃ¡n los conceptos bÃ¡sicos sobre el uso de **agentes** y cÃ³mo integrarlos con **APIs**.  \n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"../notebooks/sources/llamaindex.png\" alt=\"LlamaIndex\" style=\"width:100%; height:auto;\"/>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerrequisitos  \n",
        "\n",
        "Antes de comenzar con el curso, es necesario leer el documento [setup_instructions.md](../setup_instructions.md),  \n",
        "donde se detallan los pasos previos indispensables, como:  \n",
        "\n",
        "- InstalaciÃ³n de **Python**.  \n",
        "- InstalaciÃ³n de las **dependencias necesarias**.  \n",
        "- ObtenciÃ³n gratuita de un **token de Hugging Face** como *Inference Provider* para la ejecuciÃ³n de los LLMs en la nube.  \n",
        "- Acceso gratuito limitado a distintas APIs en tiempo real (e.g. OpenWeather API).  \n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Login en Hugging Face  \n",
        "\n",
        "Tras obtener tu **token de acceso** de Hugging Face, realizaremos el inicio de sesiÃ³n para poder utilizar las **APIs de inferencia serverless**, que se ofrecen de forma gratuita con un lÃ­mite diario de uso.  \n",
        "\n",
        "ğŸ‘‰ Puedes consultar tu consumo y lÃ­mites en: [Hugging Face â€“ Inference usage](https://huggingface.co/settings/billing)  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Â¿QuÃ© son los agentes de IA?  \n",
        "\n",
        "Los **agentes de IA** son sistemas inteligentes que pueden **razonar**, **planificar** y **actuar** de forma autÃ³noma para resolver problemas complejos. A diferencia de un chatbot tradicional que solo responde preguntas, un agente puede:\n",
        "\n",
        "- ğŸ§  **Razonar** sobre el problema que necesita resolver\n",
        "- ğŸ”§ **Utilizar herramientas** externas (APIs, bases de datos, etc.)\n",
        "- ğŸ“‹ **Planificar** una secuencia de acciones\n",
        "- ğŸ”„ **Iterar** hasta encontrar la soluciÃ³n\n",
        "\n",
        "En este curso aprenderÃ¡s a crear agentes potentes usando **LlamaIndex**, una de las librerÃ­as mÃ¡s populares para desarrollo de aplicaciones de IA.\n",
        "\n",
        "### Â¿QuÃ© vamos a aprender?\n",
        "\n",
        "1. **InteracciÃ³n bÃ¡sica** con un modelo de lenguaje\n",
        "2. **Manejo de contexto** y memoria en conversaciones\n",
        "3. **Herramientas (Tools)** - cÃ³mo dotar a los agentes de superpoderes\n",
        "4. **Agentes complejos** que pueden consultar APIs en tiempo real\n",
        "\n",
        "Â¡Empezamos! ğŸš€\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3d8a2c34ecd44f581dbd51d3f5a11f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Primeros pasos con agentes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Tu primer agente bÃ¡sico ğŸ¤–\n",
        "\n",
        "Vamos a empezar con lo mÃ¡s sencillo: crear un agente que pueda mantener una conversaciÃ³n contigo. Este agente serÃ¡ como un asistente virtual que puede entender y responder a tus preguntas.\n",
        "\n",
        "### Configurando el modelo de lenguaje\n",
        "\n",
        "Primero, vamos a configurar nuestro modelo de lenguaje usando la API gratuita de Hugging Face:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… LLM bÃ¡sico creado correctamente!\n"
          ]
        }
      ],
      "source": [
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "\n",
        "# Configure the language model\n",
        "llm = HuggingFaceInferenceAPI(\n",
        "    model_name=\"HuggingFaceTB/SmolLM3-3B\",  # A fast and efficient model\n",
        "    max_new_tokens=2048,\n",
        "    temperature=0.2  # Controls creativity: 0=deterministic, 1=very creative\n",
        ")\n",
        "\n",
        "\n",
        "print(\"âœ… LLM bÃ¡sico creado correctamente!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Â¡Probemos nuestro primer modelo!\n",
        "\n",
        "Ahora vamos a interactuar con nuestro modelo de lenguaje. Observa cÃ³mo puede responder a preguntas bÃ¡sicas:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¤– Agente: <think>\n",
            "Okay, the user sent \"Â¡Hola! Â¿CÃ³mo estÃ¡s?\" which is Spanish for \"Hello! How are you?\" I need to respond in a friendly and welcoming manner. Since the user is using Spanish, I should respond in Spanish as well to keep the conversation consistent. I should ask how they're doing and offer assistance. Let me make sure my response is polite and encourages them to ask questions or share what they need help with. Also, I should keep it concise and natural-sounding. Let me check for any grammar mistakes. \"Â¡Hola! Â¿CÃ³mo estÃ¡s? Me alegra mucho que estÃ©s bien. Â¿En quÃ© puedo ayudarte hoy?\" That sounds good. It's a greeting, a question about their well-being, and an offer to help. I think that's appropriate.\n",
            "</think>\n",
            "\n",
            "Â¡Hola! Me alegra mucho que estÃ©s bien. Â¿En quÃ© puedo ayudarte hoy?\n",
            "ğŸ¤– Agente: <think>\n",
            "Okay, the user is asking for a concise explanation of artificial intelligence in five words. Let me start by recalling the key components of AI. It's about machines that can perform tasks that usually require human intelligence. So, maybe something like \"MÃ¡quinas que imitan la inteligencia humana.\" But that's seven words. Let me shorten it.\n",
            "\n",
            "\"Inteligencia artificial\" is the term itself, but the user wants a definition. So, \"MÃ¡quinas que piensan como humanos.\" That's five words. Wait, \"piensan\" is \"think,\" but in Spanish, \"pensar\" is \"to think,\" so \"piensan\" is third person plural. Maybe \"MÃ¡quinas que imitan la inteligencia humana.\" Still seven. Hmm.\n",
            "\n",
            "Alternatively, \"Inteligencia artificial: mÃ¡quinas que aprenden y toman decisiones.\" That's six words. Maybe \"MÃ¡quinas que imitan la inteligencia humana.\" Still seven. Let me think again. The core idea is machines that can perform tasks requiring human intelligence. So, \"MÃ¡quinas que imitan la inteligencia humana.\" Maybe \"MÃ¡quinas que imitan la intelig\n"
          ]
        }
      ],
      "source": [
        "# Let's chat with our LLM!\n",
        "def chat_with_agent(message: str, llm = HuggingFaceInferenceAPI(\n",
        "    model_name=\"HuggingFaceTB/SmolLM3-3B\",  # A fast and efficient model\n",
        "    max_new_tokens=2048,\n",
        "    temperature=0.01  # Controls creativity: 0=deterministic, 1=very creative\n",
        ")):\n",
        "    \"\"\"Simple function to chat with our agent\"\"\"\n",
        "    response = llm.complete(message)\n",
        "    return response\n",
        "\n",
        "\n",
        "# Test basic conversation\n",
        "response = chat_with_agent(\"Â¡Hola! Â¿CÃ³mo estÃ¡s?\")\n",
        "print(\"ğŸ¤– Agente:\", response)\n",
        "\n",
        "response = chat_with_agent(\"Â¿Puedes explicarme quÃ© es la inteligencia artificial en 5 palabras?\")\n",
        "print(\"ğŸ¤– Agente:\", response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### ğŸ”¥ La importancia de la temperatura en los LLMs\n",
        "\n",
        "La **temperatura** es un parÃ¡metro fundamental en los modelos de lenguaje (LLMs) que controla la **aleatoriedad** y **creatividad** de las respuestas generadas. \n",
        "\n",
        "- Si la temperatura es **baja** (por ejemplo, 0.01), el modelo tiende a ser **mÃ¡s determinista** y predecible, eligiendo casi siempre la palabra mÃ¡s probable.\n",
        "- Si la temperatura es **alta** (por ejemplo, 1.0), el modelo se vuelve **mÃ¡s creativo** y puede generar respuestas mÃ¡s variadas o inesperadas.\n",
        "\n",
        "### Â¿CÃ³mo funciona la temperatura?\n",
        "\n",
        "La temperatura ajusta la probabilidad de cada palabra candidata antes de que el modelo elija la siguiente palabra. MatemÃ¡ticamente, se aplica la siguiente fÃ³rmula a las probabilidades originales (*logits*):\n",
        "\n",
        "$$\n",
        "p_i' = \\frac{e^{\\text{logit}_i / T}}{\\sum_j e^{\\text{logit}_j / T}}\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "- **logitáµ¢** â†’ puntuaciÃ³n antes de aplicar *softmax*  \n",
        "- **T** â†’ temperatura  \n",
        "- **páµ¢'** â†’ probabilidad ajustada de la palabra *i*  \n",
        "\n",
        "---\n",
        "\n",
        "- Cuando **T < 1**, las diferencias entre probabilidades se **amplifican** â†’ el modelo es mÃ¡s **determinista**.  \n",
        "- Cuando **T > 1**, las probabilidades se **suavizan** â†’ el modelo es mÃ¡s **creativo**. \n",
        "\n",
        "<center>\n",
        "  <iframe width=\"640\" height=\"360\" \n",
        "  src=\"https://www.youtube.com/embed/XsLK3tPy9SI\" \n",
        "  frameborder=\"0\" allowfullscreen></iframe>\n",
        "</center>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. AÃ±adiendo memoria y contexto : Primer Agente ğŸ§ \n",
        "\n",
        "Â¡Genial! Nuestro modelo puede responder preguntas, pero hay un problema: **no recuerda** conversaciones anteriores. Cada pregunta es como si fuera la primera vez que hablamos con Ã©l.\n",
        "\n",
        "Vamos a solucionarlo aÃ±adiendo **contexto** para que pueda mantener conversaciones mÃ¡s naturales:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ConversaciÃ³n con memoria ===\n",
            "ğŸ‘¤ Usuario: Hola, mi nombre es MarÃ­a y soy estudiante de ingenierÃ­a.\n",
            "ğŸ¤– Agente: Â¡Hola, MarÃ­a! Â¿En quÃ© Ã¡rea de ingenierÃ­a te especializas?\n",
            "\n",
            "ğŸ‘¤ Usuario: Â¿Recuerdas cuÃ¡l es mi nombre?\n",
            "ğŸ¤– Agente: SÃ­, tu nombre es MarÃ­a.\n",
            "\n",
            "ğŸ‘¤ Usuario: Â¿Y quÃ© estudio?\n",
            "ğŸ¤– Agente: Estudias ingenierÃ­a.\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.workflow import Context\n",
        "from llama_index.core.agent.workflow import AgentWorkflow\n",
        "from llama_index.llms.openai import OpenAI\n",
        "import os \n",
        "from load_dotenv import load_dotenv\n",
        "\n",
        "load_dotenv() # Sometimes its a headache, highly recommended using it!\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
        "llm_openai = OpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    api_key = OPENAI_API_KEY\n",
        "    )\n",
        "# Create an agent\n",
        "simple_agent = AgentWorkflow.from_tools_or_functions(\n",
        "    tools_or_functions=[],\n",
        "    llm=llm_openai,\n",
        "    system_prompt=(\n",
        "        \"Eres un asistente.\"\n",
        "        \"Responde siempre en espaÃ±ol y brevemente.\"\n",
        "    ),\n",
        ")\n",
        "# Create a conversation context\n",
        "ctx = Context(simple_agent)\n",
        "\n",
        "# Now let's have a conversation with memory!\n",
        "print(\"=== ConversaciÃ³n con memoria ===\")\n",
        "\n",
        "# First message - introduce ourselves\n",
        "response = await simple_agent.run(\"Hola, mi nombre es MarÃ­a y soy estudiante de ingenierÃ­a.\", ctx=ctx)\n",
        "print(\"ğŸ‘¤ Usuario: Hola, mi nombre es MarÃ­a y soy estudiante de ingenierÃ­a.\")\n",
        "print(\"ğŸ¤– Agente:\", response.response.content)\n",
        "print()\n",
        "\n",
        "# Second message - test if it remembers our name\n",
        "response = await simple_agent.run(\"Â¿Recuerdas cuÃ¡l es mi nombre?\", ctx=ctx)\n",
        "print(\"ğŸ‘¤ Usuario: Â¿Recuerdas cuÃ¡l es mi nombre?\")\n",
        "print(\"ğŸ¤– Agente:\", response.response.content)\n",
        "print()\n",
        "\n",
        "# Third message - test if it remembers our profession\n",
        "response = await simple_agent.run(\"Â¿Y quÃ© estudio?\", ctx=ctx)\n",
        "print(\"ğŸ‘¤ Usuario: Â¿Y quÃ© estudio?\")\n",
        "print(\"ğŸ¤– Agente:\", response.response.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Â¡Impresionante! ğŸ‰\n",
        "\n",
        "Como puedes ver, ahora nuestro agente **recuerda** informaciÃ³n de mensajes anteriores. Esto es fundamental para crear experiencias de conversaciÃ³n mÃ¡s naturales y Ãºtiles.\n",
        "\n",
        "**Â¿CÃ³mo funciona?**\n",
        "- El `Context` mantiene el historial de la conversaciÃ³n\n",
        "- Cada nuevo mensaje se aÃ±ade al contexto\n",
        "- El agente puede \"ver\" toda la conversaciÃ³n anterior para dar respuestas coherentes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ğŸ’¡ **Nota prÃ¡ctica**  \n",
        ">  \n",
        "> En nuestras pruebas estamos usando un modelo pequeÃ±o de Hugging Face, el **smol 3B**.  \n",
        "> Este modelo es muy ligero y rÃ¡pido, lo que lo hace perfecto para **hacer pruebas iniciales** y experimentar con ejemplos sencillos.  \n",
        "> \n",
        " \n",
        "Eso sÃ­, conviene tener en cuenta que por su tamaÃ±o reducido (unos 3 mil millones de parÃ¡metros) a veces **falla en el formato de salida**, se confunde con las instrucciones o genera resultados menos consistentes.  \n",
        "  \n",
        "En cambio, cuando utilizamos un modelo mÃ¡s moderno y de mayor escala, como los de **OpenAI (~175B parÃ¡metros estimados)**, la experiencia cambia notablemente: obtenemos respuestas mucho mÃ¡s **coherentes, fiables y consistentes**.  \n",
        " \n",
        "ğŸ‘‰ La idea es que el modelo pequeÃ±o nos sirve como **campo de pruebas rÃ¡pido**, mientras que los modelos grandes son los que realmente nos permiten **aplicar el flujo en entornos productivos** con mayor confianza.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Superpoderes para nuestro agente: Las Tools ğŸ”§\n",
        "\n",
        "Hasta ahora nuestro agente puede conversar y recordar, pero estÃ¡ limitado al conocimiento que tiene el modelo. Â¿QuÃ© pasarÃ­a si pudiera **calcular**, **buscar informaciÃ³n en internet**, o **consultar APIs**?\n",
        "\n",
        "Â¡AquÃ­ es donde entran las **Tools** (herramientas)! Las tools permiten que nuestro agente:\n",
        "\n",
        "- ğŸ§® **Calcule** operaciones matemÃ¡ticas exactas\n",
        "- ğŸŒ **Acceda** a informaciÃ³n en tiempo real\n",
        "- ğŸ“Š **Procese** datos de APIs externas\n",
        "- ğŸ” **Busque** en bases de datos\n",
        "\n",
        "### Ejemplo simple: Calculadora matemÃ¡tica\n",
        "\n",
        "Vamos a crear un agente que puede hacer cÃ¡lculos matemÃ¡ticos precisos:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ§® Agente matemÃ¡tico creado con tools: sumar, restar, multiplicar, dividir\n",
            "ğŸ§® Agente pseudo-matemÃ¡tico creado sin tools\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.agent.workflow import ToolCallResult, AgentStream\n",
        "\n",
        "# Define simple math tools\n",
        "def sumar(a: int, b: int) -> int:\n",
        "    \"\"\"Suma dos nÃºmeros enteros\"\"\"\n",
        "    return a + b\n",
        "\n",
        "def restar(a: int, b: int) -> int:\n",
        "    \"\"\"Resta dos nÃºmeros enteros\"\"\"\n",
        "    return a - b\n",
        "\n",
        "def multiplicar(a: int, b: int) -> int:\n",
        "    \"\"\"Multiplica dos nÃºmeros enteros\"\"\"\n",
        "    return a * b\n",
        "\n",
        "def dividir(a: float, b: float) -> float:\n",
        "    \"\"\"Divide dos nÃºmeros (puede devolver decimales)\"\"\"\n",
        "    if b == 0:\n",
        "        return \"Error: No se puede dividir por cero\"\n",
        "    return a / b\n",
        "\n",
        "def potencia(a:float, power:int):\n",
        "    \"\"\"Eleva un nÃºmero a la n-Ã©sima potencia\"\"\"\n",
        "    if power == 0 : \n",
        "        return 1\n",
        "    return a**power\n",
        "\n",
        "# Create an agent with math tools\n",
        "math_agent = AgentWorkflow.from_tools_or_functions(\n",
        "    tools_or_functions=[sumar, restar, multiplicar, dividir, potencia],\n",
        "    llm=llm_openai,\n",
        "    system_prompt=(\n",
        "        \"Eres un asistente matemÃ¡tico que puede realizar cÃ¡lculos exactos. \"\n",
        "        \"Utiliza las herramientas disponibles ( [sumar, restar, multiplicar, dividir, potencia] ) para hacer cÃ¡lculos precisos. \"\n",
        "        \"Responde siempre en espaÃ±ol y explica brevemente quÃ© operaciÃ³n realizaste.\"\n",
        "    ),\n",
        ")\n",
        "print(f\"ğŸ§® Agente matemÃ¡tico creado con tools: sumar, restar, multiplicar, dividir\")\n",
        "\n",
        "# Create an dumb agent without math tools\n",
        "dumb_math_agent = AgentWorkflow.from_tools_or_functions(\n",
        "    tools_or_functions=[],\n",
        "    llm=llm_openai,\n",
        "    system_prompt=(\n",
        "        \"Eres un asistente matemÃ¡tico que puede realizar cÃ¡lculos exactos. \"\n",
        "        \"Responde siempre en espaÃ±ol y explica brevemente quÃ© operaciÃ³n realizaste.\"\n",
        "    ),\n",
        ")\n",
        "print(\"ğŸ§® Agente pseudo-matemÃ¡tico creado sin tools\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Â¡Probemos las herramientas matemÃ¡ticas!\n",
        "\n",
        "Ahora vamos a ver cÃ³mo nuestro agente con herramientas puede **razonar** y **decidir** quÃ© herramienta usar para resolver problemas matemÃ¡ticos, mientras que nuestro agente simple es incapaz de resolver operaciones matemÃ¡ticas complejas:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ‘¤ Usuario: What is (2 + 2) ** 65? Give me just the result\n",
            "ğŸ¤– Agente: \n",
            "ğŸ”§ Usando herramienta: potencia con parÃ¡metros {'a': 4, 'power': 65}\n",
            "ğŸ“Š Resultado: 1361129467683753853853498429727072845824\n",
            "ğŸ¤– Agente: El resultado de \\( (2 + 2)^{65} \\) es 1361129467683753853853498429727072845824.\n",
            "==================================================\n",
            "ğŸ‘¤ Usuario: What is (2 + 2) ** 65? Give me just the result\n",
            "ğŸ¤– Agente: El resultado de \\( (2 + 2)^{65} \\) es \\( 4^{65} \\), que es igual a \\( 1,125,899,906,842,624 \\). \n",
            "\n",
            "RealicÃ© la operaciÃ³n sumando 2 + 2 para obtener 4 y luego elevÃ© 4 a la potencia de 65.\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AgentOutput(response=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='El resultado de \\\\( (2 + 2)^{65} \\\\) es \\\\( 4^{65} \\\\), que es igual a \\\\( 1,125,899,906,842,624 \\\\). \\n\\nRealicÃ© la operaciÃ³n sumando 2 + 2 para obtener 4 y luego elevÃ© 4 a la potencia de 65.')]), structured_response=None, current_agent_name='Agent', raw={'id': 'chatcmpl-C9Ihe3Lr4QBDj5pwNwOy0FXcnkd2r', 'choices': [{'delta': {'content': None, 'function_call': None, 'refusal': None, 'role': None, 'tool_calls': None}, 'finish_reason': 'stop', 'index': 0, 'logprobs': None}], 'created': 1756332882, 'model': 'gpt-4o-mini-2024-07-18', 'object': 'chat.completion.chunk', 'service_tier': 'default', 'system_fingerprint': 'fp_560af6e559', 'usage': None, 'obfuscation': 'nq'}, tool_calls=[], retry_messages=[])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Function to test math agent with streaming\n",
        "async def test_math_agent(question: str, dumb = False):\n",
        "    print(f\"ğŸ‘¤ Usuario: {question}\")\n",
        "    print(\"ğŸ¤– Agente: \", end=\"\", flush=True)\n",
        "    agent = dumb_math_agent if dumb else math_agent\n",
        "    handler = agent.run(question)\n",
        "    \n",
        "    # Stream the response and show tool calls\n",
        "    async for ev in handler.stream_events():\n",
        "        if isinstance(ev, ToolCallResult):\n",
        "            print(f\"\\nğŸ”§ Usando herramienta: {ev.tool_name} con parÃ¡metros {ev.tool_kwargs}\")\n",
        "            print(f\"ğŸ“Š Resultado: {ev.tool_output}\")\n",
        "            print(\"ğŸ¤– Agente: \", end=\"\", flush=True)\n",
        "        elif isinstance(ev, AgentStream):\n",
        "            print(ev.delta, end=\"\", flush=True)\n",
        "    \n",
        "    response = await handler\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    return response\n",
        "\n",
        "# Test various math problems\n",
        "await test_math_agent(\"What is (2 + 2) ** 65? Give me just the result\")\n",
        "await test_math_agent(\"What is (2 + 2) ** 65? Give me just the result\", dumb = True)\n",
        "# await test_math_agent(\"Si tengo 100 euros y gasto 23, Â¿cuÃ¡nto me queda?\")\n",
        "# await test_math_agent(\"Â¿CuÃ¡l es el resultado de (8 * 5) / 2?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Â¡IncreÃ­ble! ğŸ¤¯\n",
        "\n",
        "Â¿Te das cuenta de lo que acaba de pasar? Nuestro agente:\n",
        "\n",
        "1. **AnalizÃ³** la pregunta matemÃ¡tica\n",
        "2. **DecidiÃ³** quÃ© herramienta necesitaba usar\n",
        "3. **EjecutÃ³** la herramienta con los parÃ¡metros correctos\n",
        "4. **InterpretÃ³** el resultado y lo presentÃ³ de forma amigable\n",
        "\n",
        "Esto es **razonamiento automÃ¡tico** en acciÃ³n. El agente no solo ejecuta cÃ³digo, sino que **piensa** sobre quÃ© hacer y cÃ³mo hacerlo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Agente avanzado: Consultando el tiempo en tiempo real ğŸŒ¤ï¸\n",
        "\n",
        "Ahora vamos a crear algo realmente emocionante: un agente que puede consultar el **tiempo actual** de cualquier ciudad del mundo usando una API externa.\n",
        "\n",
        "Este ejemplo demuestra cÃ³mo los agentes pueden:\n",
        "- ğŸŒ Acceder a **datos en tiempo real**\n",
        "- ğŸ”„ Mantener **contexto** en conversaciones largas\n",
        "- ğŸ§  **Combinar** mÃºltiples habilidades (conversaciÃ³n + herramientas)\n",
        "\n",
        "### Configurando la herramienta del tiempo\n",
        "\n",
        "Vamos a usar la herramienta que ya tenemos creada en nuestro proyecto:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸŒ¤ï¸ Agente meteorolÃ³gico creado con acceso a datos del tiempo en tiempo real!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Add src to path to import our weather tool\n",
        "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Import our weather tool\n",
        "from src.apis.weather_api import current_weather_tool\n",
        "\n",
        "# Create the weather tool\n",
        "weather_tool = current_weather_tool()\n",
        "\n",
        "# Create an advanced agent with weather capabilities\n",
        "weather_agent = AgentWorkflow.from_tools_or_functions(\n",
        "    tools_or_functions=[weather_tool],\n",
        "    llm=llm_openai,\n",
        "    system_prompt=(\n",
        "        \"Eres un asistente meteorolÃ³gico inteligente que puede consultar el tiempo actual \"\n",
        "        \"de cualquier ciudad del mundo. Siempre responde en espaÃ±ol de manera amigable y \"\n",
        "        \"proporciona informaciÃ³n Ãºtil sobre el clima. Si no entiendes una ciudad, \"\n",
        "        \"pregunta por aclaraciÃ³n.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"ğŸŒ¤ï¸ Agente meteorolÃ³gico creado con acceso a datos del tiempo en tiempo real!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Â¡Probemos nuestro agente meteorolÃ³gico con contexto!\n",
        "\n",
        "Ahora vamos a tener una conversaciÃ³n completa con nuestro agente, donde mantenga el contexto y pueda acceder a datos del tiempo en tiempo real:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸŒ¤ï¸ Iniciando conversaciÃ³n con el agente meteorolÃ³gico...\n",
            "\n",
            "ğŸ‘¤ Usuario: Hola, me llamo Carlos y vivo en Madrid. Â¿PodrÃ­as decirme quÃ© tiempo hace hoy aquÃ­?\n",
            "ğŸ¤– Agente: \n",
            "ğŸ”§ Consultando API del tiempo...\n",
            "ğŸ“¡ ParÃ¡metros: {'city': 'Madrid,ES'}\n",
            "ğŸ“Š Datos recibidos: city='Madrid' country='ES' condition='clear sky' temperature=24.72 feels_like=24.0 humidity=29 wind_speed=5.66 units='metric'\n",
            "ğŸ¤– Agente: Â¡Hola, Carlos! Hoy en Madrid el clima es bastante agradable. Actualmente, la temperatura es de aproximadamente 24.7 Â°C, con una sensaciÃ³n tÃ©rmica de 24.0 Â°C. La humedad estÃ¡ en un 29%, y hay un viento suave de 5.66 km/h. Â¡Disfruta del dÃ­a!\n",
            "============================================================\n",
            "ğŸ‘¤ Usuario: Â¿Y quÃ© tal estÃ¡ el tiempo en Barcelona?\n",
            "ğŸ¤– Agente: \n",
            "ğŸ”§ Consultando API del tiempo...\n",
            "ğŸ“¡ ParÃ¡metros: {'city': 'Barcelona,ES'}\n",
            "ğŸ“Š Datos recibidos: city='Barcelona' country='ES' condition='light rain' temperature=25.84 feels_like=26.2 humidity=66 wind_speed=4.63 units='metric'\n",
            "ğŸ¤– Agente: En Barcelona, el clima es un poco diferente. Actualmente, hay una ligera lluvia y la temperatura es de aproximadamente 25.8 Â°C, con una sensaciÃ³n tÃ©rmica de 26.2 Â°C. La humedad estÃ¡ en un 66%, y el viento sopla a 4.63 km/h. Â¡Espero que no te afecte mucho si tienes planes al aire libre!\n",
            "============================================================\n",
            "ğŸ‘¤ Usuario: Â¿Recuerdas dÃ³nde vivo? Â¿Crees que deberÃ­a llevar paraguas hoy?\n",
            "ğŸ¤– Agente: SÃ­, recuerdo que vives en Madrid. Hoy en Madrid el clima es soleado y no hay lluvia, asÃ­ que no necesitas llevar paraguas. Â¡Disfruta de tu dÃ­a al aire libre! Si necesitas mÃ¡s informaciÃ³n, no dudes en preguntar.\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AgentOutput(response=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='SÃ­, recuerdo que vives en Madrid. Hoy en Madrid el clima es soleado y no hay lluvia, asÃ­ que no necesitas llevar paraguas. Â¡Disfruta de tu dÃ­a al aire libre! Si necesitas mÃ¡s informaciÃ³n, no dudes en preguntar.')]), structured_response=None, current_agent_name='Agent', raw={'id': 'chatcmpl-C9ImQRRhqVoImUdjhsJ8JTbDbrKNr', 'choices': [{'delta': {'content': None, 'function_call': None, 'refusal': None, 'role': None, 'tool_calls': None}, 'finish_reason': 'stop', 'index': 0, 'logprobs': None}], 'created': 1756333178, 'model': 'gpt-4o-mini-2024-07-18', 'object': 'chat.completion.chunk', 'service_tier': 'default', 'system_fingerprint': 'fp_560af6e559', 'usage': None, 'obfuscation': 'E5'}, tool_calls=[], retry_messages=[])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create conversation context for weather agent\n",
        "weather_ctx = Context(weather_agent)\n",
        "\n",
        "async def conversation_with_weather_agent(message: str, ctx):\n",
        "    \"\"\"Chat with weather agent showing the complete process\"\"\"\n",
        "    print(f\"ğŸ‘¤ Usuario: {message}\")\n",
        "    print(\"ğŸ¤– Agente: \", end=\"\", flush=True)\n",
        "    \n",
        "    handler = weather_agent.run(message, ctx=ctx)\n",
        "    \n",
        "    # Stream the response and show tool calls\n",
        "    async for ev in handler.stream_events():\n",
        "        if isinstance(ev, ToolCallResult):\n",
        "            print(f\"\\nğŸ”§ Consultando API del tiempo...\")\n",
        "            print(f\"ğŸ“¡ ParÃ¡metros: {ev.tool_kwargs}\")\n",
        "            print(f\"ğŸ“Š Datos recibidos: {ev.tool_output}\")\n",
        "            print(\"ğŸ¤– Agente: \", end=\"\", flush=True)\n",
        "        elif isinstance(ev, AgentStream):\n",
        "            print(ev.delta, end=\"\", flush=True)\n",
        "    \n",
        "    response = await handler\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    return response\n",
        "\n",
        "# Have a complete conversation with context and weather tools\n",
        "print(\"ğŸŒ¤ï¸ Iniciando conversaciÃ³n con el agente meteorolÃ³gico...\\n\")\n",
        "\n",
        "await conversation_with_weather_agent(\n",
        "    \"Hola, me llamo Carlos y vivo en Madrid. Â¿PodrÃ­as decirme quÃ© tiempo hace hoy aquÃ­?\", \n",
        "    weather_ctx\n",
        ")\n",
        "\n",
        "await conversation_with_weather_agent(\n",
        "    \"Â¿Y quÃ© tal estÃ¡ el tiempo en Barcelona?\", \n",
        "    weather_ctx\n",
        ")\n",
        "\n",
        "await conversation_with_weather_agent(\n",
        "    \"Â¿Recuerdas dÃ³nde vivo? Â¿Crees que deberÃ­a llevar paraguas hoy?\", \n",
        "    weather_ctx\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Â¡Felicidades! ğŸ‰ Has creado tu primer agente inteligente\n",
        "\n",
        "### Â¿QuÃ© hemos logrado?\n",
        "\n",
        "En este notebook hemos creado agentes progresivamente mÃ¡s sofisticados:\n",
        "\n",
        "1. **ğŸ¤– Agente bÃ¡sico**: ConversaciÃ³n simple sin herramientas\n",
        "2. **ğŸ§  Agente con memoria**: Mantiene contexto entre mensajes\n",
        "3. **ğŸ”§ Agente con tools**: Puede ejecutar herramientas matemÃ¡ticas\n",
        "4. **ğŸŒ Agente avanzado**: Accede a APIs externas en tiempo real + contexto\n",
        "\n",
        "### Â¿Por quÃ© es esto revolucionario?\n",
        "\n",
        "- **Razonamiento automÃ¡tico**: El agente decide quÃ© hacer y cuÃ¡ndo\n",
        "- **Acceso a datos externos**: No limitado al conocimiento pre-entrenado\n",
        "- **Conversaciones naturales**: Mantiene el contexto como un humano\n",
        "- **Extensibilidad**: Puedes aÃ±adir cualquier API o herramienta\n",
        "\n",
        "### Â¿QuÃ© sigue?\n",
        "\n",
        "En los prÃ³ximos notebooks aprenderÃ¡s:\n",
        "\n",
        "- âœ… âœ¨ **IntroducciÃ³n a los LLMs, agentes y tools**\n",
        "- â¬œ ğŸ“° **Agentes que buscan noticias**\n",
        "- â¬œ ğŸ“„ **Agentes que procesan documentos**\n",
        "- â¬œ ğŸ—ï¸ **Agentes multi-modales mÃ¡s complejos**\n",
        "- â¬œ ğŸš€ **Despliegue de agentes en producciÃ³n**\n",
        "\n",
        "Â¡El mundo de los agentes de IA estÃ¡ apenas comenzando! ğŸŒŸ\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
