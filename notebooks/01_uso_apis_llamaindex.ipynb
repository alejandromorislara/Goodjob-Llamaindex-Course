{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Uso de APIs con LlamaIndex**  \n",
        "\n",
        "Este notebook forma parte del curso de **IA Generativa** de la [Fundación GoodJob](https://www.fundaciongoodjob.org/).  \n",
        "En él, los alumnos aprenderán los conceptos básicos sobre el uso de **agentes** y cómo integrarlos con **APIs**.  \n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"../notebooks/sources/llamaindex.png\" alt=\"LlamaIndex\" style=\"width:100%; height:auto;\"/>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerrequisitos  \n",
        "\n",
        "Antes de comenzar con el curso, es necesario leer el documento [setup_instructions.md](../setup_instructions.md),  \n",
        "donde se detallan los pasos previos indispensables, como:  \n",
        "\n",
        "- Instalación de **Python**.  \n",
        "- Instalación de las **dependencias necesarias**.  \n",
        "- Obtención gratuita de un **token de Hugging Face** como *Inference Provider* para la ejecución de los LLMs en la nube.  \n",
        "- Acceso gratuito limitado a distintas APIs en tiempo real (e.g. OpenWeather API).  \n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Login en Hugging Face  \n",
        "\n",
        "Tras obtener tu **token de acceso** de Hugging Face, realizaremos el inicio de sesión para poder utilizar las **APIs de inferencia serverless**, que se ofrecen de forma gratuita con un límite diario de uso.  \n",
        "\n",
        "👉 Puedes consultar tu consumo y límites en: [Hugging Face – Inference usage](https://huggingface.co/settings/billing)  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ¿Qué son los agentes de IA?  \n",
        "\n",
        "Los **agentes de IA** son sistemas inteligentes que pueden **razonar**, **planificar** y **actuar** de forma autónoma para resolver problemas complejos. A diferencia de un chatbot tradicional que solo responde preguntas, un agente puede:\n",
        "\n",
        "- 🧠 **Razonar** sobre el problema que necesita resolver\n",
        "- 🔧 **Utilizar herramientas** externas (APIs, bases de datos, etc.)\n",
        "- 📋 **Planificar** una secuencia de acciones\n",
        "- 🔄 **Iterar** hasta encontrar la solución\n",
        "\n",
        "En este curso aprenderás a crear agentes potentes usando **LlamaIndex**, una de las librerías más populares para desarrollo de aplicaciones de IA.\n",
        "\n",
        "### ¿Qué vamos a aprender?\n",
        "\n",
        "1. **Interacción básica** con un modelo de lenguaje\n",
        "2. **Manejo de contexto** y memoria en conversaciones\n",
        "3. **Herramientas (Tools)** - cómo dotar a los agentes de superpoderes\n",
        "4. **Agentes complejos** que pueden consultar APIs en tiempo real\n",
        "\n",
        "¡Empezamos! 🚀\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3d8a2c34ecd44f581dbd51d3f5a11f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Primeros pasos con agentes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Tu primer agente básico 🤖\n",
        "\n",
        "Vamos a empezar con lo más sencillo: crear un agente que pueda mantener una conversación contigo. Este agente será como un asistente virtual que puede entender y responder a tus preguntas.\n",
        "\n",
        "### Configurando el modelo de lenguaje\n",
        "\n",
        "Primero, vamos a configurar nuestro modelo de lenguaje usando la API gratuita de Hugging Face:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ LLM básico creado correctamente!\n"
          ]
        }
      ],
      "source": [
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "\n",
        "# Configure the language model\n",
        "llm = HuggingFaceInferenceAPI(\n",
        "    model_name=\"HuggingFaceTB/SmolLM3-3B\",  # A fast and efficient model\n",
        "    max_new_tokens=2048,\n",
        "    temperature=0.2  # Controls creativity: 0=deterministic, 1=very creative\n",
        ")\n",
        "\n",
        "\n",
        "print(\"✅ LLM básico creado correctamente!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ¡Probemos nuestro primer modelo!\n",
        "\n",
        "Ahora vamos a interactuar con nuestro modelo de lenguaje. Observa cómo puede responder a preguntas básicas:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🤖 Agente: <think>\n",
            "Okay, the user sent \"¡Hola! ¿Cómo estás?\" which is Spanish for \"Hello! How are you?\" I need to respond in a friendly and welcoming manner. Since the user is using Spanish, I should respond in Spanish as well to keep the conversation consistent. I should ask how they're doing and offer assistance. Let me make sure my response is polite and encourages them to ask questions or share what they need help with. Also, I should keep it concise and natural-sounding. Let me check for any grammar mistakes. \"¡Hola! ¿Cómo estás? Me alegra mucho que estés bien. ¿En qué puedo ayudarte hoy?\" That sounds good. It's a greeting, a question about their well-being, and an offer to help. I think that's appropriate.\n",
            "</think>\n",
            "\n",
            "¡Hola! Me alegra mucho que estés bien. ¿En qué puedo ayudarte hoy?\n",
            "🤖 Agente: <think>\n",
            "Okay, the user is asking for a concise explanation of artificial intelligence in five words. Let me start by recalling the key components of AI. It's about machines that can perform tasks that usually require human intelligence. So, maybe something like \"Máquinas que imitan la inteligencia humana.\" But that's seven words. Let me shorten it.\n",
            "\n",
            "\"Inteligencia artificial\" is the term itself, but the user wants a definition. So, \"Máquinas que piensan como humanos.\" That's five words. Wait, \"piensan\" is \"think,\" but in Spanish, \"pensar\" is \"to think,\" so \"piensan\" is third person plural. Maybe \"Máquinas que imitan la inteligencia humana.\" Still seven. Hmm.\n",
            "\n",
            "Alternatively, \"Inteligencia artificial: máquinas que aprenden y toman decisiones.\" That's six words. Maybe \"Máquinas que imitan la inteligencia humana.\" Still seven. Let me think again. The core idea is machines that can perform tasks requiring human intelligence. So, \"Máquinas que imitan la inteligencia humana.\" Maybe \"Máquinas que imitan la intelig\n"
          ]
        }
      ],
      "source": [
        "# Let's chat with our LLM!\n",
        "def chat_with_agent(message: str, llm = HuggingFaceInferenceAPI(\n",
        "    model_name=\"HuggingFaceTB/SmolLM3-3B\",  # A fast and efficient model\n",
        "    max_new_tokens=2048,\n",
        "    temperature=0.01  # Controls creativity: 0=deterministic, 1=very creative\n",
        ")):\n",
        "    \"\"\"Simple function to chat with our agent\"\"\"\n",
        "    response = llm.complete(message)\n",
        "    return response\n",
        "\n",
        "\n",
        "# Test basic conversation\n",
        "response = chat_with_agent(\"¡Hola! ¿Cómo estás?\")\n",
        "print(\"🤖 Agente:\", response)\n",
        "\n",
        "response = chat_with_agent(\"¿Puedes explicarme qué es la inteligencia artificial en 5 palabras?\")\n",
        "print(\"🤖 Agente:\", response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 🔥 La importancia de la temperatura en los LLMs\n",
        "\n",
        "La **temperatura** es un parámetro fundamental en los modelos de lenguaje (LLMs) que controla la **aleatoriedad** y **creatividad** de las respuestas generadas. \n",
        "\n",
        "- Si la temperatura es **baja** (por ejemplo, 0.01), el modelo tiende a ser **más determinista** y predecible, eligiendo casi siempre la palabra más probable.\n",
        "- Si la temperatura es **alta** (por ejemplo, 1.0), el modelo se vuelve **más creativo** y puede generar respuestas más variadas o inesperadas.\n",
        "\n",
        "### ¿Cómo funciona la temperatura?\n",
        "\n",
        "La temperatura ajusta la probabilidad de cada palabra candidata antes de que el modelo elija la siguiente palabra. Matemáticamente, se aplica la siguiente fórmula a las probabilidades originales (*logits*):\n",
        "\n",
        "$$\n",
        "p_i' = \\frac{e^{\\text{logit}_i / T}}{\\sum_j e^{\\text{logit}_j / T}}\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "- **logitᵢ** → puntuación antes de aplicar *softmax*  \n",
        "- **T** → temperatura  \n",
        "- **pᵢ'** → probabilidad ajustada de la palabra *i*  \n",
        "\n",
        "---\n",
        "\n",
        "- Cuando **T < 1**, las diferencias entre probabilidades se **amplifican** → el modelo es más **determinista**.  \n",
        "- Cuando **T > 1**, las probabilidades se **suavizan** → el modelo es más **creativo**. \n",
        "\n",
        "<center>\n",
        "  <iframe width=\"640\" height=\"360\" \n",
        "  src=\"https://www.youtube.com/embed/XsLK3tPy9SI\" \n",
        "  frameborder=\"0\" allowfullscreen></iframe>\n",
        "</center>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Añadiendo memoria y contexto : Primer Agente 🧠\n",
        "\n",
        "¡Genial! Nuestro modelo puede responder preguntas, pero hay un problema: **no recuerda** conversaciones anteriores. Cada pregunta es como si fuera la primera vez que hablamos con él.\n",
        "\n",
        "Vamos a solucionarlo añadiendo **contexto** para que pueda mantener conversaciones más naturales:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Conversación con memoria ===\n",
            "👤 Usuario: Hola, mi nombre es María y soy estudiante de ingeniería.\n",
            "🤖 Agente: ¡Hola, María! ¿En qué área de ingeniería te especializas?\n",
            "\n",
            "👤 Usuario: ¿Recuerdas cuál es mi nombre?\n",
            "🤖 Agente: Sí, tu nombre es María.\n",
            "\n",
            "👤 Usuario: ¿Y qué estudio?\n",
            "🤖 Agente: Estudias ingeniería.\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.workflow import Context\n",
        "from llama_index.core.agent.workflow import AgentWorkflow\n",
        "from llama_index.llms.openai import OpenAI\n",
        "import os \n",
        "from load_dotenv import load_dotenv\n",
        "\n",
        "load_dotenv() # Sometimes its a headache, highly recommended using it!\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
        "llm_openai = OpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    api_key = OPENAI_API_KEY\n",
        "    )\n",
        "# Create an agent\n",
        "simple_agent = AgentWorkflow.from_tools_or_functions(\n",
        "    tools_or_functions=[],\n",
        "    llm=llm_openai,\n",
        "    system_prompt=(\n",
        "        \"Eres un asistente.\"\n",
        "        \"Responde siempre en español y brevemente.\"\n",
        "    ),\n",
        ")\n",
        "# Create a conversation context\n",
        "ctx = Context(simple_agent)\n",
        "\n",
        "# Now let's have a conversation with memory!\n",
        "print(\"=== Conversación con memoria ===\")\n",
        "\n",
        "# First message - introduce ourselves\n",
        "response = await simple_agent.run(\"Hola, mi nombre es María y soy estudiante de ingeniería.\", ctx=ctx)\n",
        "print(\"👤 Usuario: Hola, mi nombre es María y soy estudiante de ingeniería.\")\n",
        "print(\"🤖 Agente:\", response.response.content)\n",
        "print()\n",
        "\n",
        "# Second message - test if it remembers our name\n",
        "response = await simple_agent.run(\"¿Recuerdas cuál es mi nombre?\", ctx=ctx)\n",
        "print(\"👤 Usuario: ¿Recuerdas cuál es mi nombre?\")\n",
        "print(\"🤖 Agente:\", response.response.content)\n",
        "print()\n",
        "\n",
        "# Third message - test if it remembers our profession\n",
        "response = await simple_agent.run(\"¿Y qué estudio?\", ctx=ctx)\n",
        "print(\"👤 Usuario: ¿Y qué estudio?\")\n",
        "print(\"🤖 Agente:\", response.response.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ¡Impresionante! 🎉\n",
        "\n",
        "Como puedes ver, ahora nuestro agente **recuerda** información de mensajes anteriores. Esto es fundamental para crear experiencias de conversación más naturales y útiles.\n",
        "\n",
        "**¿Cómo funciona?**\n",
        "- El `Context` mantiene el historial de la conversación\n",
        "- Cada nuevo mensaje se añade al contexto\n",
        "- El agente puede \"ver\" toda la conversación anterior para dar respuestas coherentes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> 💡 **Nota práctica**  \n",
        ">  \n",
        "> En nuestras pruebas estamos usando un modelo pequeño de Hugging Face, el **smol 3B**.  \n",
        "> Este modelo es muy ligero y rápido, lo que lo hace perfecto para **hacer pruebas iniciales** y experimentar con ejemplos sencillos.  \n",
        "> \n",
        " \n",
        "Eso sí, conviene tener en cuenta que por su tamaño reducido (unos 3 mil millones de parámetros) a veces **falla en el formato de salida**, se confunde con las instrucciones o genera resultados menos consistentes.  \n",
        "  \n",
        "En cambio, cuando utilizamos un modelo más moderno y de mayor escala, como los de **OpenAI (~175B parámetros estimados)**, la experiencia cambia notablemente: obtenemos respuestas mucho más **coherentes, fiables y consistentes**.  \n",
        " \n",
        "👉 La idea es que el modelo pequeño nos sirve como **campo de pruebas rápido**, mientras que los modelos grandes son los que realmente nos permiten **aplicar el flujo en entornos productivos** con mayor confianza.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Superpoderes para nuestro agente: Las Tools 🔧\n",
        "\n",
        "Hasta ahora nuestro agente puede conversar y recordar, pero está limitado al conocimiento que tiene el modelo. ¿Qué pasaría si pudiera **calcular**, **buscar información en internet**, o **consultar APIs**?\n",
        "\n",
        "¡Aquí es donde entran las **Tools** (herramientas)! Las tools permiten que nuestro agente:\n",
        "\n",
        "- 🧮 **Calcule** operaciones matemáticas exactas\n",
        "- 🌐 **Acceda** a información en tiempo real\n",
        "- 📊 **Procese** datos de APIs externas\n",
        "- 🔍 **Busque** en bases de datos\n",
        "\n",
        "### Ejemplo simple: Calculadora matemática\n",
        "\n",
        "Vamos a crear un agente que puede hacer cálculos matemáticos precisos:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧮 Agente matemático creado con tools: sumar, restar, multiplicar, dividir\n",
            "🧮 Agente pseudo-matemático creado sin tools\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.agent.workflow import ToolCallResult, AgentStream\n",
        "\n",
        "# Define simple math tools\n",
        "def sumar(a: int, b: int) -> int:\n",
        "    \"\"\"Suma dos números enteros\"\"\"\n",
        "    return a + b\n",
        "\n",
        "def restar(a: int, b: int) -> int:\n",
        "    \"\"\"Resta dos números enteros\"\"\"\n",
        "    return a - b\n",
        "\n",
        "def multiplicar(a: int, b: int) -> int:\n",
        "    \"\"\"Multiplica dos números enteros\"\"\"\n",
        "    return a * b\n",
        "\n",
        "def dividir(a: float, b: float) -> float:\n",
        "    \"\"\"Divide dos números (puede devolver decimales)\"\"\"\n",
        "    if b == 0:\n",
        "        return \"Error: No se puede dividir por cero\"\n",
        "    return a / b\n",
        "\n",
        "def potencia(a:float, power:int):\n",
        "    \"\"\"Eleva un número a la n-ésima potencia\"\"\"\n",
        "    if power == 0 : \n",
        "        return 1\n",
        "    return a**power\n",
        "\n",
        "# Create an agent with math tools\n",
        "math_agent = AgentWorkflow.from_tools_or_functions(\n",
        "    tools_or_functions=[sumar, restar, multiplicar, dividir, potencia],\n",
        "    llm=llm_openai,\n",
        "    system_prompt=(\n",
        "        \"Eres un asistente matemático que puede realizar cálculos exactos. \"\n",
        "        \"Utiliza las herramientas disponibles ( [sumar, restar, multiplicar, dividir, potencia] ) para hacer cálculos precisos. \"\n",
        "        \"Responde siempre en español y explica brevemente qué operación realizaste.\"\n",
        "    ),\n",
        ")\n",
        "print(f\"🧮 Agente matemático creado con tools: sumar, restar, multiplicar, dividir\")\n",
        "\n",
        "# Create an dumb agent without math tools\n",
        "dumb_math_agent = AgentWorkflow.from_tools_or_functions(\n",
        "    tools_or_functions=[],\n",
        "    llm=llm_openai,\n",
        "    system_prompt=(\n",
        "        \"Eres un asistente matemático que puede realizar cálculos exactos. \"\n",
        "        \"Responde siempre en español y explica brevemente qué operación realizaste.\"\n",
        "    ),\n",
        ")\n",
        "print(\"🧮 Agente pseudo-matemático creado sin tools\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ¡Probemos las herramientas matemáticas!\n",
        "\n",
        "Ahora vamos a ver cómo nuestro agente con herramientas puede **razonar** y **decidir** qué herramienta usar para resolver problemas matemáticos, mientras que nuestro agente simple es incapaz de resolver operaciones matemáticas complejas:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "👤 Usuario: What is (2 + 2) ** 65? Give me just the result\n",
            "🤖 Agente: \n",
            "🔧 Usando herramienta: potencia con parámetros {'a': 4, 'power': 65}\n",
            "📊 Resultado: 1361129467683753853853498429727072845824\n",
            "🤖 Agente: El resultado de \\( (2 + 2)^{65} \\) es 1361129467683753853853498429727072845824.\n",
            "==================================================\n",
            "👤 Usuario: What is (2 + 2) ** 65? Give me just the result\n",
            "🤖 Agente: El resultado de \\( (2 + 2)^{65} \\) es \\( 4^{65} \\), que es igual a \\( 1,125,899,906,842,624 \\). \n",
            "\n",
            "Realicé la operación sumando 2 + 2 para obtener 4 y luego elevé 4 a la potencia de 65.\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AgentOutput(response=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='El resultado de \\\\( (2 + 2)^{65} \\\\) es \\\\( 4^{65} \\\\), que es igual a \\\\( 1,125,899,906,842,624 \\\\). \\n\\nRealicé la operación sumando 2 + 2 para obtener 4 y luego elevé 4 a la potencia de 65.')]), structured_response=None, current_agent_name='Agent', raw={'id': 'chatcmpl-C9Ihe3Lr4QBDj5pwNwOy0FXcnkd2r', 'choices': [{'delta': {'content': None, 'function_call': None, 'refusal': None, 'role': None, 'tool_calls': None}, 'finish_reason': 'stop', 'index': 0, 'logprobs': None}], 'created': 1756332882, 'model': 'gpt-4o-mini-2024-07-18', 'object': 'chat.completion.chunk', 'service_tier': 'default', 'system_fingerprint': 'fp_560af6e559', 'usage': None, 'obfuscation': 'nq'}, tool_calls=[], retry_messages=[])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Function to test math agent with streaming\n",
        "async def test_math_agent(question: str, dumb = False):\n",
        "    print(f\"👤 Usuario: {question}\")\n",
        "    print(\"🤖 Agente: \", end=\"\", flush=True)\n",
        "    agent = dumb_math_agent if dumb else math_agent\n",
        "    handler = agent.run(question)\n",
        "    \n",
        "    # Stream the response and show tool calls\n",
        "    async for ev in handler.stream_events():\n",
        "        if isinstance(ev, ToolCallResult):\n",
        "            print(f\"\\n🔧 Usando herramienta: {ev.tool_name} con parámetros {ev.tool_kwargs}\")\n",
        "            print(f\"📊 Resultado: {ev.tool_output}\")\n",
        "            print(\"🤖 Agente: \", end=\"\", flush=True)\n",
        "        elif isinstance(ev, AgentStream):\n",
        "            print(ev.delta, end=\"\", flush=True)\n",
        "    \n",
        "    response = await handler\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    return response\n",
        "\n",
        "# Test various math problems\n",
        "await test_math_agent(\"What is (2 + 2) ** 65? Give me just the result\")\n",
        "await test_math_agent(\"What is (2 + 2) ** 65? Give me just the result\", dumb = True)\n",
        "# await test_math_agent(\"Si tengo 100 euros y gasto 23, ¿cuánto me queda?\")\n",
        "# await test_math_agent(\"¿Cuál es el resultado de (8 * 5) / 2?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ¡Increíble! 🤯\n",
        "\n",
        "¿Te das cuenta de lo que acaba de pasar? Nuestro agente:\n",
        "\n",
        "1. **Analizó** la pregunta matemática\n",
        "2. **Decidió** qué herramienta necesitaba usar\n",
        "3. **Ejecutó** la herramienta con los parámetros correctos\n",
        "4. **Interpretó** el resultado y lo presentó de forma amigable\n",
        "\n",
        "Esto es **razonamiento automático** en acción. El agente no solo ejecuta código, sino que **piensa** sobre qué hacer y cómo hacerlo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Agente avanzado: Consultando el tiempo en tiempo real 🌤️\n",
        "\n",
        "Ahora vamos a crear algo realmente emocionante: un agente que puede consultar el **tiempo actual** de cualquier ciudad del mundo usando una API externa.\n",
        "\n",
        "Este ejemplo demuestra cómo los agentes pueden:\n",
        "- 🌍 Acceder a **datos en tiempo real**\n",
        "- 🔄 Mantener **contexto** en conversaciones largas\n",
        "- 🧠 **Combinar** múltiples habilidades (conversación + herramientas)\n",
        "\n",
        "### Configurando la herramienta del tiempo\n",
        "\n",
        "Vamos a usar la herramienta que ya tenemos creada en nuestro proyecto:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🌤️ Agente meteorológico creado con acceso a datos del tiempo en tiempo real!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Add src to path to import our weather tool\n",
        "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Import our weather tool\n",
        "from src.apis.weather_api import current_weather_tool\n",
        "\n",
        "# Create the weather tool\n",
        "weather_tool = current_weather_tool()\n",
        "\n",
        "# Create an advanced agent with weather capabilities\n",
        "weather_agent = AgentWorkflow.from_tools_or_functions(\n",
        "    tools_or_functions=[weather_tool],\n",
        "    llm=llm_openai,\n",
        "    system_prompt=(\n",
        "        \"Eres un asistente meteorológico inteligente que puede consultar el tiempo actual \"\n",
        "        \"de cualquier ciudad del mundo. Siempre responde en español de manera amigable y \"\n",
        "        \"proporciona información útil sobre el clima. Si no entiendes una ciudad, \"\n",
        "        \"pregunta por aclaración.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"🌤️ Agente meteorológico creado con acceso a datos del tiempo en tiempo real!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ¡Probemos nuestro agente meteorológico con contexto!\n",
        "\n",
        "Ahora vamos a tener una conversación completa con nuestro agente, donde mantenga el contexto y pueda acceder a datos del tiempo en tiempo real:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🌤️ Iniciando conversación con el agente meteorológico...\n",
            "\n",
            "👤 Usuario: Hola, me llamo Carlos y vivo en Madrid. ¿Podrías decirme qué tiempo hace hoy aquí?\n",
            "🤖 Agente: \n",
            "🔧 Consultando API del tiempo...\n",
            "📡 Parámetros: {'city': 'Madrid,ES'}\n",
            "📊 Datos recibidos: city='Madrid' country='ES' condition='clear sky' temperature=24.72 feels_like=24.0 humidity=29 wind_speed=5.66 units='metric'\n",
            "🤖 Agente: ¡Hola, Carlos! Hoy en Madrid el clima es bastante agradable. Actualmente, la temperatura es de aproximadamente 24.7 °C, con una sensación térmica de 24.0 °C. La humedad está en un 29%, y hay un viento suave de 5.66 km/h. ¡Disfruta del día!\n",
            "============================================================\n",
            "👤 Usuario: ¿Y qué tal está el tiempo en Barcelona?\n",
            "🤖 Agente: \n",
            "🔧 Consultando API del tiempo...\n",
            "📡 Parámetros: {'city': 'Barcelona,ES'}\n",
            "📊 Datos recibidos: city='Barcelona' country='ES' condition='light rain' temperature=25.84 feels_like=26.2 humidity=66 wind_speed=4.63 units='metric'\n",
            "🤖 Agente: En Barcelona, el clima es un poco diferente. Actualmente, hay una ligera lluvia y la temperatura es de aproximadamente 25.8 °C, con una sensación térmica de 26.2 °C. La humedad está en un 66%, y el viento sopla a 4.63 km/h. ¡Espero que no te afecte mucho si tienes planes al aire libre!\n",
            "============================================================\n",
            "👤 Usuario: ¿Recuerdas dónde vivo? ¿Crees que debería llevar paraguas hoy?\n",
            "🤖 Agente: Sí, recuerdo que vives en Madrid. Hoy en Madrid el clima es soleado y no hay lluvia, así que no necesitas llevar paraguas. ¡Disfruta de tu día al aire libre! Si necesitas más información, no dudes en preguntar.\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AgentOutput(response=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='Sí, recuerdo que vives en Madrid. Hoy en Madrid el clima es soleado y no hay lluvia, así que no necesitas llevar paraguas. ¡Disfruta de tu día al aire libre! Si necesitas más información, no dudes en preguntar.')]), structured_response=None, current_agent_name='Agent', raw={'id': 'chatcmpl-C9ImQRRhqVoImUdjhsJ8JTbDbrKNr', 'choices': [{'delta': {'content': None, 'function_call': None, 'refusal': None, 'role': None, 'tool_calls': None}, 'finish_reason': 'stop', 'index': 0, 'logprobs': None}], 'created': 1756333178, 'model': 'gpt-4o-mini-2024-07-18', 'object': 'chat.completion.chunk', 'service_tier': 'default', 'system_fingerprint': 'fp_560af6e559', 'usage': None, 'obfuscation': 'E5'}, tool_calls=[], retry_messages=[])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create conversation context for weather agent\n",
        "weather_ctx = Context(weather_agent)\n",
        "\n",
        "async def conversation_with_weather_agent(message: str, ctx):\n",
        "    \"\"\"Chat with weather agent showing the complete process\"\"\"\n",
        "    print(f\"👤 Usuario: {message}\")\n",
        "    print(\"🤖 Agente: \", end=\"\", flush=True)\n",
        "    \n",
        "    handler = weather_agent.run(message, ctx=ctx)\n",
        "    \n",
        "    # Stream the response and show tool calls\n",
        "    async for ev in handler.stream_events():\n",
        "        if isinstance(ev, ToolCallResult):\n",
        "            print(f\"\\n🔧 Consultando API del tiempo...\")\n",
        "            print(f\"📡 Parámetros: {ev.tool_kwargs}\")\n",
        "            print(f\"📊 Datos recibidos: {ev.tool_output}\")\n",
        "            print(\"🤖 Agente: \", end=\"\", flush=True)\n",
        "        elif isinstance(ev, AgentStream):\n",
        "            print(ev.delta, end=\"\", flush=True)\n",
        "    \n",
        "    response = await handler\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    return response\n",
        "\n",
        "# Have a complete conversation with context and weather tools\n",
        "print(\"🌤️ Iniciando conversación con el agente meteorológico...\\n\")\n",
        "\n",
        "await conversation_with_weather_agent(\n",
        "    \"Hola, me llamo Carlos y vivo en Madrid. ¿Podrías decirme qué tiempo hace hoy aquí?\", \n",
        "    weather_ctx\n",
        ")\n",
        "\n",
        "await conversation_with_weather_agent(\n",
        "    \"¿Y qué tal está el tiempo en Barcelona?\", \n",
        "    weather_ctx\n",
        ")\n",
        "\n",
        "await conversation_with_weather_agent(\n",
        "    \"¿Recuerdas dónde vivo? ¿Crees que debería llevar paraguas hoy?\", \n",
        "    weather_ctx\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ¡Felicidades! 🎉 Has creado tu primer agente inteligente\n",
        "\n",
        "### ¿Qué hemos logrado?\n",
        "\n",
        "En este notebook hemos creado agentes progresivamente más sofisticados:\n",
        "\n",
        "1. **🤖 Agente básico**: Conversación simple sin herramientas\n",
        "2. **🧠 Agente con memoria**: Mantiene contexto entre mensajes\n",
        "3. **🔧 Agente con tools**: Puede ejecutar herramientas matemáticas\n",
        "4. **🌍 Agente avanzado**: Accede a APIs externas en tiempo real + contexto\n",
        "\n",
        "### ¿Por qué es esto revolucionario?\n",
        "\n",
        "- **Razonamiento automático**: El agente decide qué hacer y cuándo\n",
        "- **Acceso a datos externos**: No limitado al conocimiento pre-entrenado\n",
        "- **Conversaciones naturales**: Mantiene el contexto como un humano\n",
        "- **Extensibilidad**: Puedes añadir cualquier API o herramienta\n",
        "\n",
        "### ¿Qué sigue?\n",
        "\n",
        "En los próximos notebooks aprenderás:\n",
        "\n",
        "- ✅ ✨ **Introducción a los LLMs, agentes y tools**\n",
        "- ⬜ 📰 **Agentes que buscan noticias**\n",
        "- ⬜ 📄 **Agentes que procesan documentos**\n",
        "- ⬜ 🏗️ **Agentes multi-modales más complejos**\n",
        "- ⬜ 🚀 **Despliegue de agentes en producción**\n",
        "\n",
        "¡El mundo de los agentes de IA está apenas comenzando! 🌟\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
